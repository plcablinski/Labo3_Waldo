{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(\"dataset_base_features.csv\", parse_dates=[\"periodo\"])\n",
    "df['product_id'] = df['product_id'].astype(str)\n",
    "\n",
    "# ---------- PROPHET FEATURES ----------\n",
    "\n",
    "def generar_features_prophet(df):\n",
    "    features = []\n",
    "\n",
    "    productos = df['product_id'].unique()\n",
    "    for pid in productos:\n",
    "        serie = df[df['product_id'] == pid][['periodo', 'tn']].rename(columns={\"periodo\": \"ds\", \"tn\": \"y\"})\n",
    "\n",
    "        if len(serie) < 4 or (serie['y'] > 0).sum() < 4:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            m = Prophet(weekly_seasonality=False, daily_seasonality=False, yearly_seasonality=True)\n",
    "            m.fit(serie)\n",
    "\n",
    "            future = m.make_future_dataframe(periods=0, freq='MS')\n",
    "            forecast = m.predict(future)\n",
    "\n",
    "            row = {\n",
    "                'product_id': pid,\n",
    "                'trend_prophet': forecast['trend'].iloc[-1],\n",
    "                'seasonal_prophet': forecast['seasonal'].iloc[-1]\n",
    "            }\n",
    "            features.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error con product_id={pid}: {e}\")\n",
    "\n",
    "    df_prophet = pd.DataFrame(features)\n",
    "    df_prophet.to_csv(\"prophet_features.csv\", index=False)\n",
    "    print(\"âœ… prophet_features.csv generado\")\n",
    "    return df_prophet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CLUSTERING CON DTW ----------\n",
    "\n",
    "def generar_clusters_dtw(df, k=50):\n",
    "    pivot = df.pivot(index=\"product_id\", columns=\"periodo\", values=\"tn\").fillna(0)\n",
    "    pivot = pivot.loc[(pivot > 0).sum(axis=1) >= 4]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(pivot)\n",
    "\n",
    "    ts_data = to_time_series_dataset(scaled)\n",
    "\n",
    "    km_dtw = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=10, random_state=42)\n",
    "    labels = km_dtw.fit_predict(ts_data)\n",
    "\n",
    "    df_cluster = pd.DataFrame({\n",
    "        'product_id': pivot.index,\n",
    "        'cluster_dtw': labels\n",
    "    })\n",
    "\n",
    "    df_cluster.to_csv(\"dtw_clusters.csv\", index=False)\n",
    "    print(\"âœ… dtw_clusters.csv generado\")\n",
    "    return df_cluster\n",
    "\n",
    "# Ejecutar funciones\n",
    "if __name__ == \"__main__\":\n",
    "    prophet_features = generar_features_prophet(df)\n",
    "    dtw_clusters = generar_clusters_dtw(df, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcebb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# === 1. Cargar datos base ===\n",
    "df = pd.read_csv(\"dataset_base_features.csv\", parse_dates=['periodo'])\n",
    "df['product_id'] = df['product_id'].astype(str)\n",
    "\n",
    "# === 2. Filtrar y preparar dataset de entrenamiento ===\n",
    "train_df = df[df['periodo'] <= '2019-10-01'].copy()\n",
    "\n",
    "# === 3. Seleccionar features ===\n",
    "exclude = ['tn', 'customer_id', 'periodo', 'product_id']\n",
    "y = train_df['tn']\n",
    "X = train_df.drop(columns=[col for col in exclude if col in train_df.columns])\n",
    "\n",
    "# === 4. DivisiÃ³n para entrenamiento ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "sample_weight = np.log1p(y_train + 1)\n",
    "\n",
    "# === 5. Crear estudio Optuna con almacenamiento SQLite ===\n",
    "storage = \"sqlite:///optuna_tn_studyww.db\"\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name=\"prediccion_tn1\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# === 6. Definir funciÃ³n objetivo ===\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "# === 7. Ejecutar optimizaciÃ³n ===\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# === 8. Entrenar modelo final con mejores hiperparÃ¡metros ===\n",
    "best_params = study.best_params\n",
    "best_params.update({'tree_method': 'hist', 'random_state': 42})\n",
    "model_final = XGBRegressor(**best_params)\n",
    "model_final.fit(X, y, sample_weight=np.log1p(y + 1))\n",
    "\n",
    "# === 9. PredicciÃ³n sobre febrero 2020 ===\n",
    "# Tomamos datos de diciembre 2019 y los \"movemos\" a febrero 2020\n",
    "febrero = df[df['periodo'] == '2019-12-01'].copy()\n",
    "febrero['periodo'] = pd.to_datetime('2020-02-01')\n",
    "febrero = febrero.groupby('product_id', as_index=False).first()\n",
    "\n",
    "# === 10. Agregar lags manuales ===\n",
    "for i, mes in zip(range(1, 4), ['2019-12-01', '2019-11-01', '2019-10-01']):\n",
    "    lag_df = (\n",
    "        df[df['periodo'] == mes]\n",
    "        .groupby('product_id', as_index=False)['tn']\n",
    "        .mean()\n",
    "        .rename(columns={'tn': f'lag_{i}'})\n",
    "    )\n",
    "    febrero = febrero.merge(lag_df, on='product_id', how='left')\n",
    "\n",
    "# === 11. Predecir y exportar (blindaje de columnas) ===\n",
    "# Rellenar con 0 cualquier feature que falte\n",
    "faltantes = set(X.columns) - set(febrero.columns)\n",
    "for col in faltantes:\n",
    "    febrero[col] = 0\n",
    "\n",
    "# Reordenar igual que en X\n",
    "febrero_X = febrero[X.columns]\n",
    "\n",
    "# Predict\n",
    "febrero['tn_pred'] = model_final.predict(febrero_X)\n",
    "\n",
    "# Exportar\n",
    "febrero[['product_id', 'tn_pred']].to_csv(\n",
    "    \"prediccion_febrero_xgb.csv\", index=False\n",
    ")\n",
    "print(\"âœ… PredicciÃ³n exportada como 'prediccion_febrero_xgb.csv'\")\n",
    "print(\"ðŸ§­ Para ver el dashboard: ejecutÃ¡ âžœ optuna-dashboard sqlite:///optuna_tn_studyww.db\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tslearn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
