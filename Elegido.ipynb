{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMER SCRIPT: REALIZA UNA REGRESIÃ“N LINEAL SIMPLE\n",
    "# Este script realiza una regresiÃ³n simple propuesta en clase.\n",
    "# Usa como variables los \"mÃ¡gicos\", que son ciertos productos seleccionados para el anÃ¡lisis.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carga del dataset de ventas\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "\n",
    "# Se agrupa por periodo y producto, sumando las toneladas vendidas (tn)\n",
    "df = df.groupby(by=[\"periodo\", \"product_id\"]).agg({\"tn\": \"sum\"}).reset_index()\n",
    "\n",
    "# Se convierte la columna 'periodo' a formato datetime\n",
    "df[\"periodo\"] = pd.to_datetime(df[\"periodo\"], format=\"%Y%m\")\n",
    "\n",
    "# ReestructuraciÃ³n del DataFrame: cada fila es un mes, cada columna un product_id, valores = tn\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 5. Lista de productos mÃ¡gicos seleccionados para el anÃ¡lisis (mÃ¡s la columna 'periodo')\n",
    "magicos = [ \n",
    "    \"periodo\", 20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021,\n",
    "    20026, 20028, 20035, 20039, 20042, 20044, 20045, 20046, 20049,\n",
    "    20051, 20052, 20053, 20055, 20008, 20001, 20017, 20086, 20180,\n",
    "    20193, 20320, 20532, 20612, 20637, 20807, 20838\n",
    "]\n",
    "\n",
    "# Se construye el conjunto de entrenamiento (X_train) con datos del aÃ±o 2018\n",
    "X_train = df_pivot[magicos].query(\"periodo >= '2018-01-01' & periodo <= '2018-12-31'\")\n",
    "X_train = X_train.T.iloc[1:]  # Transpone y elimina la fila 'periodo'\n",
    "X_train.columns = [f\"t-{11-k}\" for k in range(12)]  # Etiquetas de los 12 meses hacia atrÃ¡s\n",
    "\n",
    "# Se construye el conjunto X_kgl con los datos de 2019, para predicciÃ³n\n",
    "X_kgl = df_pivot.query(\"periodo >= '2019-01-01' & periodo <= '2019-12-31'\")\n",
    "X_kgl = X_kgl.T.iloc[1:]  # Transpone y elimina la fila 'periodo'\n",
    "X_kgl.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "\n",
    "# Se calcula el promedio por producto (fila) como baseline, por si faltan datos\n",
    "promedio = X_kgl.mean(axis=1).fillna(0)\n",
    "\n",
    "# Se crea el vector objetivo (y) usando las toneladas de febrero 2019\n",
    "y = df_pivot[magicos].query(\"periodo == '2019-02-01'\").T.iloc[1:]\n",
    "y.columns = [\"target\"]\n",
    "\n",
    "# Se eliminan productos que no tengan 12 meses de datos (tienen NaN)\n",
    "prod_menos12 = X_kgl.index[X_kgl.isna().sum(axis=1) > 0]\n",
    "X_kgl = X_kgl[~X_kgl.index.isin(prod_menos12)]  # Solo productos con info completa\n",
    "promedio_menos12 = promedio[prod_menos12]  # Guarda promedio para los que tienen datos faltantes\n",
    "\n",
    "# Se carga la lista de productos a predecir desde el archivo externo\n",
    "productos_ok = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/product_id_apredecir201912.txt\", \n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Se entrena un modelo de regresiÃ³n lineal usando X_train e y\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train, y)\n",
    "\n",
    "# Se realiza la predicciÃ³n para los productos que tienen datos completos\n",
    "pred = pd.DataFrame({\n",
    "    \"product_id\": X_kgl.index,\n",
    "    \"tn\": reg_model.predict(X_kgl).flatten()\n",
    "})\n",
    "\n",
    "# Para los productos faltantes, se completa con el promedio del aÃ±o\n",
    "nuevas_filas = []\n",
    "for prod in productos_ok[\"product_id\"]:\n",
    "    if prod not in pred[\"product_id\"].values:\n",
    "        nuevas_filas.append({\n",
    "            \"product_id\": prod, \n",
    "            \"tn\": promedio[prod]\n",
    "        })\n",
    "\n",
    "# Se unen las predicciones del modelo con los productos completados por promedio\n",
    "pred = pd.concat([pred, pd.DataFrame(nuevas_filas)], ignore_index=True)\n",
    "\n",
    "# Se aseguran solo los productos requeridos en la lista final\n",
    "pred = pred[pred[\"product_id\"].isin(productos_ok[\"product_id\"])]\n",
    "\n",
    "# ExportaciÃ³n de las predicciones a un archivo CSV \n",
    "pred.to_csv(\"prediccion_reg_lineal.csv\", index=False, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGUNDO SCRIPT: AUTOGLUON\n",
    "#El mismo script que fue compartido en zulip por Fernando Raco, tal cual lo compartiÃ³ Ã©l (solo unificado) \n",
    "# y que estuvimos viendo el clase, de probada funcionalidad. \n",
    "\n",
    "# ðŸ“¦ 1. Importar librerÃ­as\n",
    "import pandas as pd\n",
    "# ðŸ’¬ Instalar AutoGluon si es necesario\n",
    "%pip install autogluon.timeseries\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "\n",
    "# ðŸ“„ 2. Cargar datasets\n",
    "df_sellin = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
    "\n",
    "# ðŸ“„ Leer lista de productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "    \n",
    "    # ðŸ§¹ 3. Preprocesamiento\n",
    "# Convertir periodo a datetime\n",
    "df_sellin['timestamp'] = pd.to_datetime(df_sellin['periodo'], format='%Y%m')\n",
    "\n",
    "# Filtrar hasta dic 2019 y productos requeridos\n",
    "df_filtered = df_sellin[\n",
    "    (df_sellin['timestamp'] <= '2019-12-01') &\n",
    "    (df_sellin['product_id'].isin(product_ids))\n",
    "]\n",
    "\n",
    "# Agregar tn por periodo, cliente y producto\n",
    "df_grouped = df_filtered.groupby(['timestamp', 'customer_id', 'product_id'], as_index=False)['tn'].sum()\n",
    "\n",
    "# Agregar tn total por periodo y producto\n",
    "df_monthly_product = df_grouped.groupby(['timestamp', 'product_id'], as_index=False)['tn'].sum()\n",
    "\n",
    "# Agregar columna 'item_id' para AutoGluon\n",
    "df_monthly_product['item_id'] = df_monthly_product['product_id']\n",
    "\n",
    "# â° 4. Crear TimeSeriesDataFrame\n",
    "ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_monthly_product,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Completar valores faltantes\n",
    "ts_data = ts_data.fill_missing_values()\n",
    "\n",
    "# âš™ï¸ 5. Definir y entrenar predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=2,\n",
    "    target='tn',\n",
    "    freq='MS'  # Frecuencia mensual (Month Start), \n",
    ")\n",
    "\n",
    "predictor.fit(ts_data, num_val_windows=2, time_limit=60*60)\n",
    "\n",
    "# ðŸ”® 6. Generar predicciÃ³n\n",
    "forecast = predictor.predict(ts_data)\n",
    "\n",
    "# Extraer predicciÃ³n media y filtrar febrero 2020\n",
    "forecast_mean = forecast['mean'].reset_index()\n",
    "print(forecast_mean.columns)\n",
    "\n",
    "# Tomar solo item_id y la predicciÃ³n 'mean'\n",
    "resultado = forecast['mean'].reset_index()[['item_id', 'mean']]\n",
    "resultado.columns = ['product_id', 'tn']\n",
    "\n",
    "# Filtrar solo febrero 2020\n",
    "resultado = forecast['mean'].reset_index()\n",
    "resultado = resultado[resultado['timestamp'] == '2020-02-01']\n",
    "\n",
    "# Renombrar columnas\n",
    "resultado = resultado[['item_id', 'mean']]\n",
    "resultado.columns = ['product_id', 'tn']\n",
    "\n",
    "\n",
    "# ðŸ’¾ 7. Guardar archivo\n",
    "resultado.to_csv(\"predicciones_febrero2020_fecha_01_07.csv\", index=False)\n",
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TERCER SCRIPT: COMPARATIVA DE MODELOS\n",
    "# Este script ejecuta una comparativa de modelos, con validaciÃ³n ampliada a septiembre, octubre y noviembre. 0.260 en el public\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# Se inicializa la salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "# Se crea carpeta par autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# Se filtran los datos del df, ordenÃ¡ndolos por fecha y extrayendo el mes como Ãºnica feature.\n",
    "# Se dividen los datos en un conjunto de entrenamiento (todo antes de septiembre de 2019) y otro de validaciÃ³n (solo septiembre, octubre y noviembre de 2019)\n",
    "# Se preparan los datasets X_train y X_val usando Ãºnicamente el mes como variable explicativa, y y_train y y_val con las toneladas vendidas. \n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # Se aplica el primer modelo de RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # Se aplica el segundo modelo: ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # Se aplica el tercer modelo, LightGBM. Los hiperparÃ¡metros son ajustados en funciÃ³n de una optimizaciÃ³n previa con Optuna.\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # Se aplica el cuarto modelo: XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # Se aplica el quinto modelo, AutoGluon, tomando como base el script que se habÃ­a probado funcional en clase.\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "   # Este bloque selecciona el mejor modelo, basÃ¡ndose en el error MAE.  \n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Para los productos que no pudieron ser modelados en el paso anterior (por falta de datos o por error)\n",
    "# se aplica una estrategia de fallback para asegurar que todos los productos tengan una predicciÃ³n.\n",
    "# Se calcula el promedio de las toneladas vendidas en los Ãºltimos 12 meses antes de septiembre\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Se guardan los resultados en archivos csv \n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto3.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo.csv\", index=False)\n",
    "with open(\"log_modelos3.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUARTO SCRIPT: ENSEMBLE DE RESULTADOS\n",
    "# Este script combina las predicciones de varios modelos para obtener una predicciÃ³n final promediada. Se utilizan para ello\n",
    "# los acrchivos generados por los scripts anteriores. Y se combinan con un cuarto, que es el resultado del modelo elegido por Pablo Cablinski.\n",
    "# de ese modo se presenta un csv que es el elegido por el grupo para la entrega final del laboratorio. En el public performa 0.242\n",
    "\n",
    "# Cargar archivos\n",
    "df1 = pd.read_csv(\"predicciones_febrero2020_porproducto3.csv\")\n",
    "df2 = pd.read_csv(\"prediccion_reg_lineal.csv\")\n",
    "df3 = pd.read_csv(\"lgbm_predictions_median.csv\")\n",
    "df4 = pd.read_csv(\"predicciones_febrero2020_autogluonFernando.csv\")\n",
    "\n",
    "\n",
    "# Renombrar columnas para consistencia\n",
    "df1.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df2.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df3.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df4.columns = [\"product_id\", \"tn_predicho\"]\n",
    "\n",
    "\n",
    "# Unir por product_id\n",
    "df_merge = df1.merge(df2, on=\"product_id\", suffixes=(\"_1\", \"_2\"))\n",
    "df_merge = df_merge.merge(df3, on=\"product_id\")\n",
    "df_merge.rename(columns={\"tn_predicho\": \"tn_predicho_3\"}, inplace=True)\n",
    "df_merge = df_merge.merge(df4, on=\"product_id\")\n",
    "df_merge.rename(columns={\"tn_predicho\": \"tn_predicho_4\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Calcular promedio\n",
    "df_merge[\"tn_predicho\"] = df_merge[\n",
    "    [\"tn_predicho_1\", \"tn_predicho_2\", \"tn_predicho_3\", \"tn_predicho_4\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Mergear las predicciones del modelo\n",
    "df_merge[\"tn_predicho\"] \n",
    "\n",
    "# Seleccionar columnas finales\n",
    "df_final = df_merge[[\"product_id\", \"tn_predicho\"]]\n",
    "\n",
    "# Guardar archivo final\n",
    "df_final.to_csv(\"predicciones_promediadas_pw.csv\", index=False)\n",
    "print(\"Archivo guardado como predicciones_promediadas_pw.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predprod1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
